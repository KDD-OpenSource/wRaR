{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pdb 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:11:49.977549Z",
     "start_time": "2017-06-28T08:11:49.611125Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import csrar\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:11:50.497167Z",
     "start_time": "2017-06-28T08:11:50.489519Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('../data/page-blocks0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T21:41:51.431380Z",
     "start_time": "2017-06-27T21:41:51.414851Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For rootkit_imap...\n",
    "for cat_col in ['Atr-1', 'Atr-2', 'Atr-3']:\n",
    "    data[cat_col] = pd.Categorical(data[cat_col])\n",
    "    data[cat_col] = data[cat_col].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:11:52.074507Z",
     "start_time": "2017-06-28T08:11:52.068394Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# For page-block0.csv, convert 'positive'/'negative' to 0 and 1\n",
    "data['Class'] = pd.Categorical(data['Class'])\n",
    "data['Class'] = data['Class'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T21:44:40.104596Z",
     "start_time": "2017-06-27T21:44:40.093703Z"
    }
   },
   "outputs": [],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:02:42.702379Z",
     "start_time": "2017-06-28T08:02:42.693594Z"
    }
   },
   "outputs": [],
   "source": [
    "uniques = np.unique(data['Class'], return_counts=True)\n",
    "perc = list(map(lambda c: c/len(data), uniques[1]))\n",
    "perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T15:54:26.957577Z",
     "start_time": "2017-06-27T15:54:26.940880Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import arff\n",
    "file = open('../data/page-blocks0.arff', 'r')\n",
    "dataset = arff.load(file)\n",
    "data = pd.DataFrame(dataset['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### About test2.arff:\n",
    "useful=Vector(0, 1)  \n",
    "dependent=Vector(5, 6, 7, 8, 9)  \n",
    "duplicated=Vector((0,12), (4,10), (6,11))  \n",
    "nominal=List()  \n",
    "clusters=[(0, 1, 2), (3, 4)]  \n",
    "distribution=List(24860, 25140)  \n",
    "seed=0\n",
    "#### About test3.arff\n",
    "useful=Vector(0, 1)  \n",
    "dependent=Vector(5, 6, 7, 8, 9)  \n",
    "duplicated=Vector((0,12), (4,10), (6,11))  \n",
    "nominal=List()  \n",
    "clusters=[(0, 1, 2), (3, 4)]  \n",
    "distribution=List(249786, 250214)  \n",
    "#### About test4.arff\n",
    "useful=Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29)  \n",
    "dependent=Vector(100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112, 113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138, 139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149)  \n",
    "duplicated=Vector((7,168), (10,157), (24,159), (28,162), (30,166), (50,169), (61,154), (82,158), (99,167), (104,156), (107,151), (113,164), (117,155), (117,165), (124,150), (135,160), (139,152), (145,161), (146,153), (147,163))  \n",
    "nominal=List()  \n",
    "clusters=[(0, 1, 2, 3, 4, 5, 6, 7, 8, 9), (10, 11, 12, 13, 14), (15, 16, 17)]  \n",
    "distribution=List(250085, 249915)  \n",
    "#### About test5.arff\n",
    "useful=Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9)  \n",
    "dependent=Vector(80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 102, 103, 104, 105, 106, 107, 108, 109)  \n",
    "duplicated=Vector((4,118), (8,114), (8,146), (9,137), (9,144), (10,131), (17,123), (22,116), (24,120), (24,142), (25,125), (35,127), (42,119), (43,122), (45,111), (48,134), (50,132), (52,110), (54,138), (57,112), (57,115), (57,149), (62,141), (64,136), (70,117), (70,148), (73,126), (76,133), (77,140), (78,143), (83,113), (84,139), (91,145), (92,121), (93,128), (93,135), (95,124), (103,129), (104,130), (105,147))  \n",
    "nominal=List()  \n",
    "clusters=[(0, 1, 2), (3, 4), (5, 6, 7, 8, 9, 10, 11, 12, 13, 14)]  \n",
    "distribution=List(99718, 100282)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:11:54.827688Z",
     "start_time": "2017-06-28T08:11:54.819262Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = 'Class'\n",
    "X = data.drop(target, axis=1)\n",
    "y = data[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RaR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-28T08:12:02.130Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevance: 100.00%    \n",
      "Running optimizer...\n",
      "Optimizer done.\n",
      "Redundancy: 100.00%    \n",
      "1. Area with a score of 1.0\n",
      "2. P_and with a score of 0.9558130381842868\n",
      "3. Blackand with a score of 0.8516152992762337\n",
      "4. Mean_tr with a score of 0.8208340765981941\n",
      "5. Wb_trans with a score of 0.8007923632096755\n",
      "6. P_black with a score of 0.7674271827074405\n",
      "7. Length with a score of 0.7582465944365564\n",
      "8. Height with a score of 0.6618303073626751\n",
      "9. Eccen with a score of 0.6154548465809933\n",
      "10. Blackpix with a score of 0.5524184187938588\n",
      "Relevance: 100.00%    \n",
      "Running optimizer...\n",
      "Optimizer done.\n",
      "Redundancy: 100.00%    \n",
      "1. Wb_trans with a score of 1.0\n",
      "2. Length with a score of 0.9581995971778461\n",
      "3. Blackpix with a score of 0.8874623696032095\n",
      "4. Height with a score of 0.8668753522782068\n",
      "5. Eccen with a score of 0.8668753520673685\n",
      "6. P_and with a score of 0.8526697513675702\n",
      "7. Area with a score of 0.8359599269598075\n",
      "8. P_black with a score of 0.7311360006626799\n",
      "9. Mean_tr with a score of 0.6856697798081576\n",
      "10. Blackand with a score of 0.6104483315300925\n",
      "Relevance: 100.00%    \n",
      "Running optimizer...\n",
      "Optimizer done.\n",
      "Redundancy: 100.00%    \n",
      "1. Height with a score of 1.0\n",
      "2. Area with a score of 0.9688861285195912\n",
      "3. Wb_trans with a score of 0.9688861237708236\n",
      "4. Eccen with a score of 0.9631847380139116\n",
      "5. Mean_tr with a score of 0.9139869788570434\n",
      "6. Length with a score of 0.9029857247791879\n",
      "7. P_and with a score of 0.8725170186554568\n",
      "8. P_black with a score of 0.8021246759426589\n",
      "9. Blackand with a score of 0.6525262041299356\n",
      "10. Blackpix with a score of 0.6237349985982473\n",
      "Relevance: 100.00%    \n",
      "Running optimizer...\n",
      "Optimizer done.\n",
      "Redundancy: 100.00%    \n",
      "1. Mean_tr with a score of 1.0\n",
      "2. Height with a score of 0.9123816343320794\n",
      "3. P_and with a score of 0.905402858746616\n",
      "4. P_black with a score of 0.8524296157535284\n",
      "5. Wb_trans with a score of 0.8384104955981603\n",
      "6. Area with a score of 0.8365471278863728\n",
      "7. Blackand with a score of 0.8365471254017781\n",
      "8. Eccen with a score of 0.7134369351053402\n",
      "9. Blackpix with a score of 0.5119073698577747\n",
      "10. Length with a score of 0.4403263397126536\n",
      "Relevance: 100.00%    \n",
      "Running optimizer...\n",
      "Optimizer done.\n",
      "Redundancy: 100.00%    \n",
      "1. Eccen with a score of 1.0\n",
      "2. Blackand with a score of 0.909595105992678\n",
      "3. P_and with a score of 0.8161255308668038\n",
      "4. Mean_tr with a score of 0.7863919128148943\n",
      "5. P_black with a score of 0.7720643671668899\n",
      "6. Height with a score of 0.7460285214247199\n",
      "7. Wb_trans with a score of 0.7380771479472394\n",
      "8. Length with a score of 0.5951001824042673\n",
      "9. Area with a score of 0.5758671402713964\n",
      "10. Blackpix with a score of 0.5124533249731078\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.029406</td>\n",
       "      <td>0.183966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1\n",
       "0  0.029406  0.183966"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_frames = 0\n",
    "for i in range(5):\n",
    "    rar_data = pd.concat([X_train, y_train], axis=1)\n",
    "    # RaR uses columns with string label only\n",
    "    rar_data.rename(columns=lambda c: str(c), inplace=True)\n",
    "    target = str(target)\n",
    "    rar = csrar.rar.RaR(rar_data)\n",
    "    rar.run(target, k=3, runs=100, split_iterations=10, compensate_imbalance=False)\n",
    "    error_frame = eval_rar_ranking(rar.feature_ranking, k=4)\n",
    "    avg_frames += error_frame\n",
    "avg_frames /= 5\n",
    "avg_frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-28T08:12:17.154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated cost matrix:\n",
      "          0          1\n",
      "0  1.108747  10.195652\n",
      "Overall cost matrix:\n",
      "          0          1\n",
      "0  1.108747  10.195652\n",
      "Relevance: 100.00%    \n",
      "Running optimizer...\n",
      "Optimizer done.\n",
      "Redundancy: 100.00%    \n",
      "1. Wb_trans with a score of 1.0\n",
      "2. Length with a score of 0.9999999843719695\n",
      "3. P_and with a score of 0.943696777902834\n",
      "4. Height with a score of 0.9313989377967021\n",
      "5. Area with a score of 0.9071012431113519\n",
      "6. Mean_tr with a score of 0.9071012274039609\n",
      "7. Blackand with a score of 0.9071012244466945\n",
      "8. P_black with a score of 0.9071012244462633\n",
      "9. Eccen with a score of 0.9071012244460956\n",
      "10. Blackpix with a score of 0.9071012244459642\n",
      "Generated cost matrix:\n",
      "          0          1\n",
      "0  1.108747  10.195652\n",
      "Overall cost matrix:\n",
      "          0          1\n",
      "0  1.108747  10.195652\n",
      "Relevance: 100.00%    \n",
      "Running optimizer...\n",
      "Optimizer done.\n",
      "Redundancy: 100.00%    \n",
      "1. Length with a score of 1.0\n",
      "2. Blackand with a score of 0.9999999999999808\n",
      "3. P_black with a score of 0.9260353907223305\n",
      "4. Mean_tr with a score of 0.9260353907216416\n",
      "5. P_and with a score of 0.9260353907016268\n",
      "6. Wb_trans with a score of 0.9260353907012084\n",
      "7. Eccen with a score of 0.9260353906969951\n",
      "8. Height with a score of 0.9260353906969202\n",
      "9. Area with a score of 0.9260353906933219\n",
      "10. Blackpix with a score of 0.9260353906928029\n",
      "Generated cost matrix:\n",
      "          0          1\n",
      "0  1.108747  10.195652\n",
      "Overall cost matrix:\n",
      "          0          1\n",
      "0  1.108747  10.195652\n",
      "Relevance: 100.00%    \n",
      "Running optimizer...\n",
      "Optimizer done.\n",
      "Redundancy: 100.00%    \n",
      "1. Area with a score of 1.0\n",
      "2. Wb_trans with a score of 0.9999999953192592\n",
      "3. Length with a score of 0.9054150171237754\n",
      "4. Height with a score of 0.9054150115083914\n",
      "5. P_black with a score of 0.9054150115081845\n",
      "6. Blackpix with a score of 0.9054150115079993\n",
      "7. Mean_tr with a score of 0.9054150115075849\n",
      "8. P_and with a score of 0.905415011507549\n",
      "9. Blackand with a score of 0.9054150115050543\n",
      "10. Eccen with a score of 0.9054150115043701\n",
      "Generated cost matrix:\n",
      "          0          1\n",
      "0  1.108747  10.195652\n",
      "Overall cost matrix:\n",
      "          0          1\n",
      "0  1.108747  10.195652\n",
      "Relevance: 100.00%    \n",
      "Running optimizer...\n",
      "Optimizer done.\n",
      "Redundancy: 100.00%    \n",
      "1. Length with a score of 1.0\n",
      "2. Wb_trans with a score of 0.9999999999978243\n",
      "3. P_and with a score of 0.9111652427040503\n",
      "4. P_black with a score of 0.8847454789853776\n",
      "5. Height with a score of 0.8847454789651144\n",
      "6. Mean_tr with a score of 0.8847454789591345\n",
      "7. Area with a score of 0.8847454789482506\n",
      "8. Eccen with a score of 0.8847454789465021\n",
      "9. Blackand with a score of 0.8847454789456427\n",
      "10. Blackpix with a score of 0.8847454789447816\n",
      "Generated cost matrix:\n",
      "          0          1\n",
      "0  1.108747  10.195652\n",
      "Overall cost matrix:\n",
      "          0          1\n",
      "0  1.108747  10.195652\n",
      "Relevance: 100.00%    \n",
      "Running optimizer...\n",
      "Optimizer done.\n",
      "Redundancy: 70.00%     "
     ]
    }
   ],
   "source": [
    "avg_frames2 = 0\n",
    "for i in range(5):\n",
    "    rar_data = pd.concat([X_train, y_train], axis=1)\n",
    "    # RaR uses columns with string label only\n",
    "    rar_data.rename(columns=lambda c: str(c), inplace=True)\n",
    "    target = str(target)\n",
    "    rar_compensate = csrar.rar.RaR(rar_data)\n",
    "    rar_compensate.run(target, k=3, runs=100, split_iterations=10, compensate_imbalance=True)\n",
    "    error_frame2 = eval_rar_ranking(rar_compensate.feature_ranking, k=4)\n",
    "    avg_frames2 += error_frame2\n",
    "avg_frames2 /= 5\n",
    "avg_frames2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T16:07:47.409552Z",
     "start_time": "2017-06-27T16:07:47.407584Z"
    }
   },
   "source": [
    "### With selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:11:56.877135Z",
     "start_time": "2017-06-28T08:11:56.843519Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_rar_ranking(ranking, k=3):\n",
    "    from imblearn.over_sampling import SMOTE\n",
    "    \n",
    "    clf_selected = ExtraTreesClassifier(max_features=k, n_jobs=-1)\n",
    "    selected_features = list(map(lambda f: f[0], ranking[:k]))\n",
    "    X_train_s = X_train[selected_features]\n",
    "    X_test_s = X_test[selected_features]\n",
    "    \n",
    "    # Rebalance\n",
    "    sm = SMOTE(random_state=42)\n",
    "    X_res, y_res = sm.fit_sample(X_train_s, y_train)\n",
    "    \n",
    "    clf_selected.fit(X_res, y_res)\n",
    "    y_predict = clf_selected.predict(X_test_s)\n",
    "    error_rates = dict([(value, {'sum': 0, 'errors': 0}) for value in np.unique(data['Class'])])\n",
    "    for real, predicted in zip(y_test, y_predict):\n",
    "        if real != predicted:\n",
    "            error_rates[real]['errors'] += 1\n",
    "        error_rates[real]['sum'] += 1\n",
    "\n",
    "    error_frame = pd.DataFrame(index=[0], columns=np.unique(data['Class']))\n",
    "    for value, rates in error_rates.items():\n",
    "        error_frame[value] = [rates['errors'] / (rates['sum'])]\n",
    "    return error_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-27T16:06:29.802362Z",
     "start_time": "2017-06-27T16:06:29.798890Z"
    }
   },
   "source": [
    "### Without selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:03:21.231401Z",
     "start_time": "2017-06-28T08:03:21.197836Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Resampling for imbalanced learn\n",
    "from imblearn.over_sampling import SMOTE \n",
    "\n",
    "sm = SMOTE(random_state=42)\n",
    "X_res, y_res = sm.fit_sample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:03:22.926748Z",
     "start_time": "2017-06-28T08:03:22.693352Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = ExtraTreesClassifier(n_jobs=-1)\n",
    "clf.fit(X_res, y_res)\n",
    "\n",
    "y_predict = clf.predict(X_test)\n",
    "error_rates = dict([(value, {'sum': 0, 'errors': 0}) for value in np.unique(data['Class'])])\n",
    "for real, predicted in zip(y_test, y_predict):\n",
    "    if real != predicted:\n",
    "        error_rates[real]['errors'] += 1\n",
    "    error_rates[real]['sum'] += 1\n",
    "error_frame = pd.DataFrame(index=[0], columns=np.unique(data['Class']))\n",
    "for value, rates in error_rates.items():\n",
    "    error_frame[value] = [rates['errors'] / (rates['sum'])]\n",
    "error_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:03:26.581688Z",
     "start_time": "2017-06-28T08:03:26.476375Z"
    }
   },
   "outputs": [],
   "source": [
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DecisionTreeClassifier for Feature Ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-06-28T08:03:18.983932Z",
     "start_time": "2017-06-28T08:03:18.964759Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-08T12:19:21.148Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ranking = []\n",
    "for index, importance in enumerate(clf.feature_importances_):\n",
    "    ranking.append((data.columns[index], importance))\n",
    "ranking.sort(key=lambda r: r[1], reverse=True)\n",
    "for (index, rank) in enumerate(ranking):\n",
    "        print('{}. {} with a score of {}'.format(index + 1, rank[0], rank[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Benchmarking with top k features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-08T12:19:24.460Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "scores = pd.DataFrame(columns=['RaR', 'DecisionTree'], index=np.arange(40))\n",
    "scores = scores.fillna(0)\n",
    "max_k = 40\n",
    "\n",
    "# RaR\n",
    "for i in range(1,max_k):\n",
    "    clf_selected = ExtraTreesClassifier(max_features=i, n_jobs=-1)\n",
    "    selected_features = list(map(lambda f: int(f[0]), rar.feature_ranking[:i]))\n",
    "    X_train_s = X_train[selected_features]\n",
    "    X_test_s = X_test[selected_features]\n",
    "    clf_selected.fit(X_train_s, y_train)\n",
    "    scores.loc[i, 'RaR'] = clf_selected.score(X_test_s, y_test)\n",
    "    sys.stdout.write('\\rBenchmark: {}%     '.format(100 * i / (2*max_k)))\n",
    "    sys.stdout.flush()\n",
    "    \n",
    "# DecisionTree\n",
    "for i in range(1,max_k):\n",
    "    clf_selected = ExtraTreesClassifier(max_features=i, n_jobs=-1)\n",
    "    selected_features = [r[0] for r in ranking[:i]]\n",
    "    X_train_s = X_train[selected_features]\n",
    "    X_test_s = X_test[selected_features]\n",
    "    clf_selected.fit(X_train_s, y_train)\n",
    "    scores.loc[i, 'DecisionTree'] = clf_selected.score(X_test_s, y_test)\n",
    "    sys.stdout.write('\\rBenchmark: {}%     '.format(50 + 100 * i / (2*max_k)))\n",
    "    sys.stdout.flush()\n",
    "print('Benchmark: 100.0%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2017-06-08T12:19:27.588Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(scores['DecisionTree'], label='DecisionTree')\n",
    "plt.plot(scores['RaR'], label='RaR')\n",
    "plt.legend(bbox_to_anchor=(0., 1.02, 1., .102), loc=3,\n",
    "           ncol=2, mode=\"expand\", borderaxespad=0.)\n",
    "plt.savefig('test4_FS_ExtraTrees_Score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# In case we need to look at the tree\n",
    "from sklearn import tree\n",
    "import pydot\n",
    "from sklearn.externals.six import StringIO\n",
    "dot_data = StringIO()\n",
    "tree.export_graphviz(clf, out_file='tree.dot')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "notify_time": "30"
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
